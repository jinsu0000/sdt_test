[INFO] Using device: cuda
Filter the characters containing more than max_len points
number of test images:  12634
Content_TR:: __init__ d_model: 512 , nhead: 2 , num_encoder_layers: 3 , dim_feedforward: 2048 , dropout: 0.1 , activation: relu , normalize_before: True
[TransformerDecoderLayer]: d_model=512, nhead=8, dim_feedforward=2048, dropout=0.1, activation=relu, normalize_before=True
/data/samsung/jinsu0000/9_conda/CASHG/miniconda3/envs/sdt_test/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/data/samsung/jinsu0000/9_conda/CASHG/miniconda3/envs/sdt_test/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/data/samsung/jinsu0000/9_conda/CASHG/miniconda3/envs/sdt_test/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
TransformerDecoder:: __init__ decoder_layer: TransformerDecoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (multihead_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (linear1): Linear(in_features=512, out_features=2048, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=2048, out_features=512, bias=True)
  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
  (dropout3): Dropout(p=0.1, inplace=False)
) , num_layers: 2 , norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True) , return_intermediate: True
TransformerDecoder:: __init__ decoder_layer: TransformerDecoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (multihead_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (linear1): Linear(in_features=512, out_features=2048, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=2048, out_features=512, bias=True)
  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
  (dropout3): Dropout(p=0.1, inplace=False)
) , num_layers: 2 , norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True) , return_intermediate: True
load pretrained model from Saved/English_CASIA/English_log-20251124_075009/model/checkpoint-iter199999.pth
Type of checkpoint['model']: <class 'collections.OrderedDict'>
0: module.Feat_Encoder.0.weight
1: module.Feat_Encoder.1.weight
2: module.Feat_Encoder.1.bias
3: module.Feat_Encoder.1.running_mean
4: module.Feat_Encoder.1.running_var
5: module.Feat_Encoder.1.num_batches_tracked
6: module.Feat_Encoder.4.0.conv1.weight
7: module.Feat_Encoder.4.0.bn1.weight
8: module.Feat_Encoder.4.0.bn1.bias
9: module.Feat_Encoder.4.0.bn1.running_mean
load pretrained model from Saved/English_CASIA/English_log-20251124_075009/model/checkpoint-iter199999.pth
  0%|          | 0/198 [00:00<?, ?it/s]SDT_Generator::inference style_imgs: torch.Size([64, 15, 1, 64, 64])
SDT_Generator::inference style_imgs.view(): torch.Size([960, 1, 64, 64])
SDT_Generator::inference Feat_Encoder_ResNet output: torch.Size([960, 512, 2, 2])
SDT_Generator::inference FEAT_ST: torch.Size([4, 960, 512])
Content_TR:: Feat_Encoder input: torch.Size([64, 1, 64, 64])
Content_TR:: Feat_Encoder Feat_Encoder(conv2d+resnet) output: torch.Size([64, 512, 2, 2])
Content_TR:: rearrange output: torch.Size([4, 64, 512])
Content_TR:: encoder output: torch.Size([4, 64, 512])
generate_square_subsequent_mask: torch.Size([121, 121])
[TransformerDecoder] *** forward: memory.shape=[1][B][torch.Size([60, 64, 512])], tgt.shape=[C][B][torch.Size([121, 64, 512])]
[TransformerDecoderLayer] *** forward_pre: memory.shape=[1][B][512], tgt.shape=[C][B][512]
[TransformerDecoderLayer] forward_pre tgt2.shape=[C][B][512] after self_attn
[TransformerDecoderLayer] forward_pre tgt.shape=[C][B][512] after sum with dropout1, norm2
[TransformerDecoderLayer] forward_pre tgt2.shape=[C][B][512] after multihead_attn
[TransformerDecoderLayer] *** forward_pre tgt.shape=[C][B][512] after linear
[TransformerDecoder] forward: layer[0] output.shape=[C][B][512]
[TransformerDecoder] forward: layer[1] output.shape=[C][B][512]
[TransformerDecoder] *** forward: output=[C][B][torch.Size([121, 64, 512])]
  1%|          | 1/198 [00:03<12:29,  3.81s/it]  1%|          | 2/198 [00:05<07:44,  2.37s/it]  2%|▏         | 3/198 [00:06<06:14,  1.92s/it]  2%|▏         | 4/198 [00:08<05:42,  1.77s/it]  3%|▎         | 5/198 [00:09<05:31,  1.72s/it]  3%|▎         | 6/198 [00:11<05:07,  1.60s/it]  4%|▎         | 7/198 [00:12<04:52,  1.53s/it]  4%|▍         | 8/198 [00:13<04:42,  1.49s/it]  5%|▍         | 9/198 [00:15<04:36,  1.46s/it]  5%|▌         | 10/198 [00:16<04:31,  1.44s/it]  6%|▌         | 11/198 [00:18<04:27,  1.43s/it]  6%|▌         | 12/198 [00:19<04:23,  1.42s/it]  7%|▋         | 13/198 [00:20<04:23,  1.42s/it]  7%|▋         | 14/198 [00:22<04:18,  1.41s/it]  8%|▊         | 15/198 [00:23<04:18,  1.42s/it]  8%|▊         | 16/198 [00:25<04:15,  1.41s/it]  9%|▊         | 17/198 [00:26<04:14,  1.41s/it]  9%|▉         | 18/198 [00:27<04:11,  1.40s/it] 10%|▉         | 19/198 [00:29<04:10,  1.40s/it] 10%|█         | 20/198 [00:30<04:08,  1.40s/it] 11%|█         | 21/198 [00:32<04:07,  1.40s/it] 11%|█         | 22/198 [00:33<04:06,  1.40s/it] 12%|█▏        | 23/198 [00:34<04:04,  1.40s/it] 12%|█▏        | 24/198 [00:36<04:03,  1.40s/it] 13%|█▎        | 25/198 [00:37<04:01,  1.40s/it] 13%|█▎        | 26/198 [00:39<04:01,  1.40s/it] 14%|█▎        | 27/198 [00:40<04:00,  1.41s/it] 14%|█▍        | 28/198 [00:41<03:58,  1.41s/it] 15%|█▍        | 29/198 [00:43<03:55,  1.40s/it] 15%|█▌        | 30/198 [00:44<03:54,  1.40s/it] 16%|█▌        | 31/198 [00:46<03:52,  1.39s/it] 16%|█▌        | 32/198 [00:47<03:52,  1.40s/it] 17%|█▋        | 33/198 [00:48<03:52,  1.41s/it] 17%|█▋        | 34/198 [00:50<03:49,  1.40s/it] 18%|█▊        | 35/198 [00:51<03:47,  1.39s/it] 18%|█▊        | 36/198 [00:53<03:46,  1.40s/it] 19%|█▊        | 37/198 [00:54<03:45,  1.40s/it] 19%|█▉        | 38/198 [00:55<03:44,  1.41s/it] 20%|█▉        | 39/198 [00:57<03:41,  1.40s/it] 20%|██        | 40/198 [00:58<03:40,  1.40s/it] 21%|██        | 41/198 [01:00<03:39,  1.40s/it] 21%|██        | 42/198 [01:01<03:37,  1.39s/it] 22%|██▏       | 43/198 [01:02<03:37,  1.40s/it] 22%|██▏       | 44/198 [01:04<03:36,  1.41s/it] 23%|██▎       | 45/198 [01:05<03:33,  1.40s/it] 23%|██▎       | 46/198 [01:07<03:37,  1.43s/it] 24%|██▎       | 47/198 [01:08<03:34,  1.42s/it] 24%|██▍       | 48/198 [01:09<03:32,  1.41s/it] 25%|██▍       | 49/198 [01:11<03:30,  1.41s/it] 25%|██▌       | 50/198 [01:12<03:27,  1.41s/it] 26%|██▌       | 51/198 [01:14<03:26,  1.40s/it] 26%|██▋       | 52/198 [01:15<03:25,  1.40s/it] 27%|██▋       | 53/198 [01:16<03:23,  1.40s/it] 27%|██▋       | 54/198 [01:18<03:21,  1.40s/it] 28%|██▊       | 55/198 [01:19<03:21,  1.41s/it] 28%|██▊       | 56/198 [01:21<03:18,  1.39s/it] 29%|██▉       | 57/198 [01:22<03:17,  1.40s/it] 29%|██▉       | 58/198 [01:24<03:20,  1.43s/it] 30%|██▉       | 59/198 [01:25<03:17,  1.42s/it] 30%|███       | 60/198 [01:26<03:15,  1.42s/it] 31%|███       | 61/198 [01:28<03:12,  1.40s/it] 31%|███▏      | 62/198 [01:29<03:11,  1.41s/it] 32%|███▏      | 63/198 [01:31<03:09,  1.40s/it] 32%|███▏      | 64/198 [01:32<03:07,  1.40s/it] 33%|███▎      | 65/198 [01:33<03:06,  1.40s/it] 33%|███▎      | 66/198 [01:35<03:05,  1.41s/it] 34%|███▍      | 67/198 [01:36<03:03,  1.40s/it] 34%|███▍      | 68/198 [01:38<03:03,  1.41s/it] 35%|███▍      | 69/198 [01:39<03:00,  1.40s/it] 35%|███▌      | 70/198 [01:40<02:58,  1.40s/it] 36%|███▌      | 71/198 [01:42<02:58,  1.41s/it] 36%|███▋      | 72/198 [01:43<02:56,  1.40s/it] 37%|███▋      | 73/198 [01:45<02:54,  1.39s/it] 37%|███▋      | 74/198 [01:46<02:53,  1.40s/it] 38%|███▊      | 75/198 [01:47<02:52,  1.40s/it] 38%|███▊      | 76/198 [01:49<02:51,  1.40s/it] 39%|███▉      | 77/198 [01:50<02:49,  1.40s/it] 39%|███▉      | 78/198 [01:52<02:47,  1.40s/it] 40%|███▉      | 79/198 [01:53<02:46,  1.40s/it] 40%|████      | 80/198 [01:55<02:48,  1.43s/it] 41%|████      | 81/198 [01:56<02:46,  1.43s/it] 41%|████▏     | 82/198 [01:57<02:44,  1.42s/it] 42%|████▏     | 83/198 [01:59<02:41,  1.40s/it] 42%|████▏     | 84/198 [02:00<02:41,  1.41s/it] 43%|████▎     | 85/198 [02:01<02:38,  1.40s/it] 43%|████▎     | 86/198 [02:03<02:37,  1.40s/it] 44%|████▍     | 87/198 [02:04<02:35,  1.40s/it] 44%|████▍     | 88/198 [02:06<02:34,  1.40s/it] 45%|████▍     | 89/198 [02:07<02:31,  1.39s/it] 45%|████▌     | 90/198 [02:09<02:31,  1.40s/it] 46%|████▌     | 91/198 [02:10<02:29,  1.39s/it] 46%|████▋     | 92/198 [02:11<02:28,  1.40s/it] 47%|████▋     | 93/198 [02:13<02:27,  1.40s/it] 47%|████▋     | 94/198 [02:14<02:26,  1.41s/it] 48%|████▊     | 95/198 [02:16<02:27,  1.43s/it] 48%|████▊     | 96/198 [02:17<02:24,  1.42s/it] 49%|████▉     | 97/198 [02:19<02:26,  1.45s/it] 49%|████▉     | 98/198 [02:20<02:22,  1.43s/it] 50%|█████     | 99/198 [02:21<02:21,  1.43s/it] 51%|█████     | 100/198 [02:23<02:17,  1.40s/it] 51%|█████     | 101/198 [02:24<02:17,  1.41s/it] 52%|█████▏    | 102/198 [02:25<02:14,  1.40s/it] 52%|█████▏    | 103/198 [02:27<02:13,  1.41s/it] 53%|█████▎    | 104/198 [02:28<02:12,  1.41s/it] 53%|█████▎    | 105/198 [02:30<02:10,  1.40s/it] 54%|█████▎    | 106/198 [02:31<02:07,  1.39s/it] 54%|█████▍    | 107/198 [02:32<02:07,  1.41s/it] 55%|█████▍    | 108/198 [02:34<02:05,  1.40s/it] 55%|█████▌    | 109/198 [02:35<02:04,  1.40s/it] 56%|█████▌    | 110/198 [02:37<02:03,  1.40s/it] 56%|█████▌    | 111/198 [02:38<02:02,  1.40s/it] 57%|█████▋    | 112/198 [02:39<01:59,  1.39s/it] 57%|█████▋    | 113/198 [02:41<01:59,  1.40s/it] 58%|█████▊    | 114/198 [02:42<01:57,  1.40s/it] 58%|█████▊    | 115/198 [02:44<01:56,  1.40s/it] 59%|█████▊    | 116/198 [02:45<01:54,  1.40s/it] 59%|█████▉    | 117/198 [02:46<01:53,  1.40s/it] 60%|█████▉    | 118/198 [02:48<01:51,  1.40s/it] 60%|██████    | 119/198 [02:49<01:51,  1.41s/it] 61%|██████    | 120/198 [02:51<01:49,  1.40s/it] 61%|██████    | 121/198 [02:52<01:47,  1.40s/it] 62%|██████▏   | 122/198 [02:54<01:46,  1.40s/it] 62%|██████▏   | 123/198 [02:55<01:44,  1.40s/it] 63%|██████▎   | 124/198 [02:56<01:43,  1.39s/it] 63%|██████▎   | 125/198 [02:58<01:42,  1.40s/it] 64%|██████▎   | 126/198 [02:59<01:41,  1.41s/it] 64%|██████▍   | 127/198 [03:00<01:39,  1.40s/it] 65%|██████▍   | 128/198 [03:02<01:38,  1.40s/it] 65%|██████▌   | 129/198 [03:03<01:36,  1.39s/it] 66%|██████▌   | 130/198 [03:05<01:36,  1.42s/it] 66%|██████▌   | 131/198 [03:06<01:35,  1.42s/it] 67%|██████▋   | 132/198 [03:08<01:33,  1.42s/it] 67%|██████▋   | 133/198 [03:09<01:31,  1.41s/it] 68%|██████▊   | 134/198 [03:10<01:30,  1.41s/it] 68%|██████▊   | 135/198 [03:12<01:28,  1.41s/it] 69%|██████▊   | 136/198 [03:13<01:26,  1.40s/it] 69%|██████▉   | 137/198 [03:15<01:25,  1.40s/it] 70%|██████▉   | 138/198 [03:16<01:23,  1.40s/it] 70%|███████   | 139/198 [03:17<01:22,  1.40s/it] 71%|███████   | 140/198 [03:19<01:21,  1.40s/it] 71%|███████   | 141/198 [03:20<01:19,  1.40s/it] 72%|███████▏  | 142/198 [03:22<01:18,  1.40s/it] 72%|███████▏  | 143/198 [03:23<01:17,  1.40s/it] 73%|███████▎  | 144/198 [03:24<01:15,  1.40s/it] 73%|███████▎  | 145/198 [03:26<01:14,  1.41s/it] 74%|███████▎  | 146/198 [03:27<01:12,  1.40s/it] 74%|███████▍  | 147/198 [03:29<01:11,  1.40s/it] 75%|███████▍  | 148/198 [03:30<01:09,  1.39s/it] 75%|███████▌  | 149/198 [03:31<01:08,  1.40s/it] 76%|███████▌  | 150/198 [03:33<01:07,  1.40s/it] 76%|███████▋  | 151/198 [03:34<01:05,  1.40s/it] 77%|███████▋  | 152/198 [03:36<01:04,  1.40s/it] 77%|███████▋  | 153/198 [03:37<01:03,  1.40s/it] 78%|███████▊  | 154/198 [03:38<01:01,  1.40s/it] 78%|███████▊  | 155/198 [03:40<00:59,  1.39s/it] 79%|███████▉  | 156/198 [03:41<00:59,  1.41s/it] 79%|███████▉  | 157/198 [03:43<00:57,  1.40s/it] 80%|███████▉  | 158/198 [03:44<00:55,  1.40s/it] 80%|████████  | 159/198 [03:45<00:54,  1.41s/it] 81%|████████  | 160/198 [03:47<00:53,  1.40s/it] 81%|████████▏ | 161/198 [03:48<00:51,  1.40s/it] 82%|████████▏ | 162/198 [03:50<00:50,  1.40s/it] 82%|████████▏ | 163/198 [03:51<00:50,  1.43s/it] 83%|████████▎ | 164/198 [03:52<00:48,  1.42s/it] 83%|████████▎ | 165/198 [03:54<00:47,  1.44s/it] 84%|████████▍ | 166/198 [03:55<00:45,  1.43s/it] 84%|████████▍ | 167/198 [03:57<00:44,  1.43s/it] 85%|████████▍ | 168/198 [03:58<00:42,  1.41s/it] 85%|████████▌ | 169/198 [04:00<00:41,  1.42s/it] 86%|████████▌ | 170/198 [04:01<00:39,  1.41s/it] 86%|████████▋ | 171/198 [04:02<00:37,  1.41s/it] 87%|████████▋ | 172/198 [04:04<00:37,  1.44s/it] 87%|████████▋ | 173/198 [04:05<00:35,  1.42s/it] 88%|████████▊ | 174/198 [04:07<00:33,  1.41s/it] 88%|████████▊ | 175/198 [04:08<00:32,  1.40s/it] 89%|████████▉ | 176/198 [04:09<00:30,  1.41s/it] 89%|████████▉ | 177/198 [04:11<00:29,  1.40s/it] 90%|████████▉ | 178/198 [04:12<00:28,  1.41s/it] 90%|█████████ | 179/198 [04:14<00:26,  1.40s/it] 91%|█████████ | 180/198 [04:15<00:25,  1.40s/it] 91%|█████████▏| 181/198 [04:16<00:23,  1.40s/it] 92%|█████████▏| 182/198 [04:18<00:22,  1.40s/it] 92%|█████████▏| 183/198 [04:19<00:20,  1.40s/it] 93%|█████████▎| 184/198 [04:21<00:19,  1.40s/it] 93%|█████████▎| 185/198 [04:22<00:18,  1.40s/it] 94%|█████████▍| 186/198 [04:23<00:16,  1.40s/it] 94%|█████████▍| 187/198 [04:25<00:15,  1.40s/it] 95%|█████████▍| 188/198 [04:26<00:14,  1.40s/it] 95%|█████████▌| 189/198 [04:28<00:12,  1.40s/it] 96%|█████████▌| 190/198 [04:29<00:11,  1.40s/it] 96%|█████████▋| 191/198 [04:30<00:09,  1.40s/it] 97%|█████████▋| 192/198 [04:32<00:08,  1.41s/it] 97%|█████████▋| 193/198 [04:33<00:07,  1.40s/it] 98%|█████████▊| 194/198 [04:35<00:05,  1.40s/it] 98%|█████████▊| 195/198 [04:36<00:04,  1.41s/it] 99%|█████████▉| 196/198 [04:37<00:02,  1.40s/it] 99%|█████████▉| 197/198 [04:39<00:01,  1.40s/it]SDT_Generator::inference style_imgs: torch.Size([26, 15, 1, 64, 64])
SDT_Generator::inference style_imgs.view(): torch.Size([390, 1, 64, 64])
SDT_Generator::inference Feat_Encoder_ResNet output: torch.Size([390, 512, 2, 2])
SDT_Generator::inference FEAT_ST: torch.Size([4, 390, 512])
Content_TR:: Feat_Encoder input: torch.Size([26, 1, 64, 64])
Content_TR:: Feat_Encoder Feat_Encoder(conv2d+resnet) output: torch.Size([26, 512, 2, 2])
Content_TR:: rearrange output: torch.Size([4, 26, 512])
Content_TR:: encoder output: torch.Size([4, 26, 512])
[TransformerDecoder] *** forward: memory.shape=[1][B][torch.Size([60, 26, 512])], tgt.shape=[C][B][torch.Size([121, 26, 512])]
[TransformerDecoder] *** forward: output=[C][B][torch.Size([121, 26, 512])]
100%|██████████| 198/198 [04:40<00:00,  1.27s/it]100%|██████████| 198/198 [04:40<00:00,  1.42s/it]
