[SEED] 1001
Filter the characters containing more than max_len points
Filter the characters containing more than max_len points
Number of train images: 50557 | test images: 12634
Content_TR:: __init__ d_model: 512 , nhead: 2 , num_encoder_layers: 3 , dim_feedforward: 2048 , dropout: 0.1 , activation: relu , normalize_before: True
[TransformerDecoderLayer]: d_model=512, nhead=8, dim_feedforward=2048, dropout=0.1, activation=relu, normalize_before=True
TransformerDecoder:: __init__ decoder_layer: TransformerDecoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (multihead_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (linear1): Linear(in_features=512, out_features=2048, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=2048, out_features=512, bias=True)
  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
  (dropout3): Dropout(p=0.1, inplace=False)
) , num_layers: 2 , norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True) , return_intermediate: True
TransformerDecoder:: __init__ decoder_layer: TransformerDecoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (multihead_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (linear1): Linear(in_features=512, out_features=2048, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=2048, out_features=512, bias=True)
  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
  (dropout3): Dropout(p=0.1, inplace=False)
) , num_layers: 2 , norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True) , return_intermediate: True
SDT_FlowWrapper:: __init__ H: 6, stride_default: 4, n_layers: 6, n_head: 8, ffn_mult: 4, p: 0.1
[TB] writing to: Saved/English_CASIA/flow_run-20251011_192043/tboard
[INFO] max_iter: 300000
/home/jinsu0000/anaconda3/envs/exp-sdt/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/jinsu0000/anaconda3/envs/exp-sdt/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Content_TR:: Feat_Encoder input: torch.Size([32, 1, 64, 64])
Content_TR:: Feat_Encoder Feat_Encoder(conv2d+resnet) output: torch.Size([32, 512, 2, 2])
Content_TR:: rearrange output: torch.Size([4, 32, 512])
Content_TR:: encoder output: torch.Size([4, 32, 512])
SDT_FlowWrapper::_content_token char_img: torch.Size([32, 1, 64, 64]), torch.float32 -> cont: torch.Size([32, 512]), torch.float32
SDT_FlowWrapper::flow_match_loss B: 32, T: 33, C: 5, H: 6, stride: 4, cond_ctx: torch.Size([32, 3, 512]), torch.float32
SDT_FlowWrapper::flow_match_loss B=32 T=33 H=6 stride=4 | EOC%=100.0 | first_eoc(mean)=31.0
iter:0 loss:4.198 ETA:60 days, 23:03:11 flow:2.717 pen:1.061 wNCE:4.114 gNCE:4.292 pen_acc:0.477 wW:0.050 wG:0.050
iter:1 loss:214.585 ETA:30 days, 11:24:41
iter:2 loss:12.631 ETA:29 days, 18:03:39
iter:3 loss:77.313 ETA:30 days, 9:37:37
iter:4 loss:24.208 ETA:29 days, 16:38:44
iter:5 loss:18.035 ETA:30 days, 1:07:01
iter:6 loss:8.857 ETA:30 days, 4:48:14
iter:7 loss:3.138 ETA:31 days, 15:05:09
iter:8 loss:2.992 ETA:30 days, 1:06:04
iter:9 loss:1.879 ETA:29 days, 23:05:05
iter:10 loss:2.687 ETA:30 days, 10:47:31 flow:1.861 pen:0.459 wNCE:4.117 gNCE:3.226 pen_acc:0.984 wW:0.050 wG:0.050
iter:11 loss:1.495 ETA:30 days, 12:34:07
iter:12 loss:3.356 ETA:30 days, 21:52:25
iter:13 loss:3.251 ETA:30 days, 7:22:02
iter:14 loss:2.201 ETA:30 days, 10:56:26
iter:15 loss:1.853 ETA:30 days, 8:49:05
iter:16 loss:1.916 ETA:30 days, 18:07:43
iter:17 loss:2.148 ETA:30 days, 19:16:26
iter:18 loss:1.368 ETA:30 days, 8:48:22
iter:19 loss:2.529 ETA:30 days, 8:54:13
iter:20 loss:3.421 ETA:30 days, 15:06:05 flow:2.622 pen:0.433 wNCE:4.069 gNCE:3.252 pen_acc:0.986 wW:0.050 wG:0.050
iter:21 loss:2.292 ETA:31 days, 0:08:50
iter:22 loss:1.582 ETA:30 days, 10:10:03
iter:23 loss:1.797 ETA:30 days, 12:14:36
iter:24 loss:1.706 ETA:30 days, 9:30:03
iter:25 loss:1.708 ETA:30 days, 9:15:55
iter:26 loss:1.290 ETA:30 days, 15:39:17
iter:27 loss:1.063 ETA:30 days, 19:39:53
iter:28 loss:1.054 ETA:30 days, 8:34:21
iter:29 loss:1.083 ETA:30 days, 13:14:01
iter:30 loss:1.164 ETA:30 days, 9:17:56 flow:0.481 pen:0.354 wNCE:4.051 gNCE:2.541 pen_acc:0.990 wW:0.050 wG:0.050
iter:31 loss:1.426 ETA:30 days, 10:06:22
iter:32 loss:1.090 ETA:31 days, 0:17:13
iter:33 loss:1.082 ETA:30 days, 13:08:51
iter:34 loss:1.153 ETA:30 days, 10:11:17
iter:35 loss:1.100 ETA:30 days, 14:37:45
iter:36 loss:1.042 ETA:30 days, 9:16:36
iter:37 loss:0.886 ETA:30 days, 8:58:55
iter:38 loss:0.980 ETA:30 days, 12:20:13
iter:39 loss:0.818 ETA:31 days, 0:16:59
iter:40 loss:1.283 ETA:30 days, 8:54:13 flow:0.592 pen:0.362 wNCE:4.139 gNCE:2.438 pen_acc:0.986 wW:0.050 wG:0.050
iter:41 loss:1.516 ETA:30 days, 7:59:22
iter:42 loss:1.110 ETA:30 days, 13:05:39
iter:43 loss:0.923 ETA:30 days, 9:41:14
iter:44 loss:1.108 ETA:30 days, 9:37:45
iter:45 loss:0.910 ETA:31 days, 3:50:26
iter:46 loss:0.954 ETA:30 days, 9:21:18
iter:47 loss:1.027 ETA:30 days, 11:01:32
iter:48 loss:0.799 ETA:30 days, 13:42:29
iter:49 loss:0.997 ETA:30 days, 9:54:20
iter:50 loss:0.970 ETA:30 days, 11:34:08 flow:0.306 pen:0.365 wNCE:4.118 gNCE:1.861 pen_acc:0.982 wW:0.050 wG:0.050
iter:51 loss:0.907 ETA:31 days, 4:42:16
iter:52 loss:1.019 ETA:30 days, 9:13:51
iter:53 loss:0.704 ETA:30 days, 10:10:52
iter:54 loss:1.309 ETA:30 days, 12:08:54
iter:55 loss:2.012 ETA:30 days, 9:28:12
iter:56 loss:1.236 ETA:30 days, 10:45:27
iter:57 loss:0.743 ETA:31 days, 3:49:39
iter:58 loss:1.038 ETA:30 days, 7:02:33
iter:59 loss:1.223 ETA:30 days, 9:31:46
iter:60 loss:1.053 ETA:30 days, 12:23:13 flow:0.494 pen:0.271 wNCE:4.028 gNCE:1.739 pen_acc:0.988 wW:0.050 wG:0.050
iter:61 loss:0.946 ETA:30 days, 8:20:39
iter:62 loss:0.801 ETA:30 days, 14:59:01
iter:63 loss:0.842 ETA:30 days, 23:45:20
iter:64 loss:0.875 ETA:30 days, 10:14:32
iter:65 loss:0.753 ETA:30 days, 9:51:16
iter:66 loss:0.710 ETA:30 days, 13:25:57
iter:67 loss:0.707 ETA:30 days, 9:37:56
iter:68 loss:0.819 ETA:30 days, 10:57:14
iter:69 loss:0.873 ETA:30 days, 22:22:35
iter:70 loss:0.640 ETA:30 days, 8:37:24 flow:0.102 pen:0.238 wNCE:4.075 gNCE:1.919 pen_acc:0.987 wW:0.050 wG:0.050
iter:71 loss:0.938 ETA:30 days, 7:14:47
iter:72 loss:1.123 ETA:30 days, 8:50:26
iter:73 loss:0.941 ETA:30 days, 11:57:54
iter:74 loss:0.704 ETA:30 days, 17:36:52
iter:75 loss:0.739 ETA:30 days, 11:47:27
iter:76 loss:0.773 ETA:30 days, 12:08:21
iter:77 loss:0.618 ETA:30 days, 7:27:37
iter:78 loss:0.694 ETA:30 days, 10:21:04
iter:79 loss:0.689 ETA:30 days, 15:23:09
iter:80 loss:0.703 ETA:30 days, 22:22:25 flow:0.182 pen:0.236 wNCE:4.113 gNCE:1.594 pen_acc:0.985 wW:0.050 wG:0.050
iter:81 loss:0.549 ETA:30 days, 11:03:04
iter:82 loss:0.765 ETA:30 days, 15:15:12
iter:83 loss:0.671 ETA:30 days, 11:05:21
iter:84 loss:0.558 ETA:30 days, 9:22:16
iter:85 loss:0.640 ETA:30 days, 14:48:16
iter:86 loss:0.582 ETA:30 days, 18:46:30
iter:87 loss:0.675 ETA:30 days, 12:34:18
iter:88 loss:0.717 ETA:30 days, 11:38:24
iter:89 loss:0.685 ETA:30 days, 8:06:52
iter:90 loss:0.511 ETA:30 days, 8:31:31 flow:0.098 pen:0.137 wNCE:4.118 gNCE:1.419 pen_acc:0.993 wW:0.050 wG:0.050
iter:91 loss:0.569 ETA:30 days, 13:19:49
iter:92 loss:0.593 ETA:30 days, 8:04:32
iter:93 loss:0.551 ETA:30 days, 22:47:11
iter:94 loss:0.578 ETA:30 days, 13:17:32
iter:95 loss:0.593 ETA:30 days, 8:10:46
iter:96 loss:0.590 ETA:30 days, 8:56:57
iter:97 loss:0.523 ETA:30 days, 14:11:34
iter:98 loss:0.550 ETA:30 days, 7:50:05
iter:99 loss:0.524 ETA:30 days, 22:42:37
SDT_FlowWrapper::flow_infer style_imgs: torch.Size([1, 30, 1, 64, 64]), torch.float32, B: 1, char_img: torch.Size([1, 1, 64, 64]), torch.float32, T: 120, steps: 20, stride: 4, replan: 4, solver: euler, micro_pen_ensemble: False, micro_pen_weight: linear
Content_TR:: Feat_Encoder input: torch.Size([1, 1, 64, 64])
Content_TR:: Feat_Encoder Feat_Encoder(conv2d+resnet) output: torch.Size([1, 512, 2, 2])
Content_TR:: rearrange output: torch.Size([4, 1, 512])
Content_TR:: encoder output: torch.Size([4, 1, 512])
SDT_FlowWrapper::_content_token char_img: torch.Size([1, 1, 64, 64]), torch.float32 -> cont: torch.Size([1, 512]), torch.float32
[INF-chunk0] h=6, a.std=0.9494, pen_prob(mean)=[0.9777266979217529, 0.01241239719092846, 0.009860923513770103]
trainer_flow::train tb_sample() delta5.size(-1) = 5
iter:100 loss:0.607 ETA:31 days, 11:32:56 flow:0.152 pen:0.202 wNCE:4.091 gNCE:0.984 pen_acc:0.987 wW:0.050 wG:0.050
iter:101 loss:0.525 ETA:32 days, 4:26:23
iter:102 loss:0.557 ETA:30 days, 10:07:08
iter:103 loss:0.509 ETA:30 days, 11:31:53
iter:104 loss:0.551 ETA:30 days, 15:25:22
iter:105 loss:0.586 ETA:30 days, 23:49:15
iter:106 loss:0.541 ETA:30 days, 10:41:10
iter:107 loss:0.468 ETA:30 days, 14:13:36
iter:108 loss:0.436 ETA:30 days, 10:15:54
iter:109 loss:0.530 ETA:30 days, 11:27:04
iter:110 loss:0.493 ETA:30 days, 15:52:31 flow:0.055 pen:0.185 wNCE:4.040 gNCE:1.013 pen_acc:0.986 wW:0.050 wG:0.050
iter:111 loss:0.508 ETA:30 days, 23:04:44
iter:112 loss:0.546 ETA:30 days, 9:58:49
iter:113 loss:0.510 ETA:30 days, 13:24:06
iter:114 loss:0.465 ETA:30 days, 9:02:42
iter:115 loss:0.553 ETA:30 days, 9:50:16
iter:116 loss:0.497 ETA:30 days, 15:08:05
iter:117 loss:0.566 ETA:30 days, 21:03:03
iter:118 loss:0.532 ETA:30 days, 14:00:19
iter:119 loss:0.429 ETA:30 days, 14:55:32
iter:120 loss:0.514 ETA:30 days, 10:44:18 flow:0.026 pen:0.248 wNCE:3.962 gNCE:0.843 pen_acc:0.981 wW:0.050 wG:0.050
iter:121 loss:0.481 ETA:30 days, 11:48:35
iter:122 loss:0.510 ETA:30 days, 14:59:48
iter:123 loss:0.528 ETA:31 days, 0:30:02
iter:124 loss:0.469 ETA:30 days, 10:26:44
iter:125 loss:0.503 ETA:30 days, 14:18:27
iter:126 loss:0.467 ETA:30 days, 11:42:16
iter:127 loss:0.457 ETA:30 days, 13:15:43
iter:128 loss:0.480 ETA:30 days, 20:09:28
iter:129 loss:0.461 ETA:30 days, 21:25:41
iter:130 loss:0.478 ETA:30 days, 12:57:34 flow:0.064 pen:0.173 wNCE:3.858 gNCE:0.960 pen_acc:0.987 wW:0.050 wG:0.050
iter:131 loss:0.553 ETA:30 days, 15:03:58
iter:132 loss:0.408 ETA:30 days, 11:20:24
iter:133 loss:0.489 ETA:30 days, 11:05:57
iter:134 loss:0.487 ETA:30 days, 11:23:14
iter:135 loss:0.504 ETA:31 days, 4:53:36
iter:136 loss:0.511 ETA:30 days, 11:00:51
iter:137 loss:0.457 ETA:30 days, 11:37:43
iter:138 loss:0.469 ETA:30 days, 14:32:20
iter:139 loss:0.450 ETA:30 days, 11:30:31
iter:140 loss:0.495 ETA:30 days, 10:31:35 flow:0.061 pen:0.198 wNCE:3.942 gNCE:0.773 pen_acc:0.983 wW:0.050 wG:0.050
iter:141 loss:0.436 ETA:31 days, 5:37:57
iter:142 loss:0.449 ETA:30 days, 10:47:23
iter:143 loss:0.433 ETA:30 days, 10:43:51
iter:144 loss:0.512 ETA:30 days, 15:59:17
iter:145 loss:0.568 ETA:30 days, 12:19:10
iter:146 loss:0.427 ETA:30 days, 10:39:31
iter:147 loss:0.492 ETA:31 days, 5:10:26
iter:148 loss:0.527 ETA:30 days, 11:55:18
iter:149 loss:0.534 ETA:30 days, 10:20:39
iter:150 loss:0.526 ETA:30 days, 15:32:05 flow:0.103 pen:0.184 wNCE:4.003 gNCE:0.775 pen_acc:0.982 wW:0.050 wG:0.050
iter:151 loss:0.582 ETA:30 days, 11:10:55
iter:152 loss:0.477 ETA:30 days, 10:09:37
iter:153 loss:0.481 ETA:31 days, 5:51:26
iter:154 loss:0.575 ETA:30 days, 10:33:52
iter:155 loss:0.440 ETA:30 days, 10:37:18
iter:156 loss:0.531 ETA:30 days, 14:55:48
iter:157 loss:0.494 ETA:30 days, 11:12:52
iter:158 loss:0.404 ETA:30 days, 10:18:53
iter:159 loss:0.539 ETA:31 days, 5:50:07
iter:160 loss:0.653 ETA:30 days, 10:00:53 flow:0.254 pen:0.177 wNCE:3.989 gNCE:0.440 pen_acc:0.986 wW:0.050 wG:0.050
iter:161 loss:0.534 ETA:30 days, 11:26:02
iter:162 loss:0.411 ETA:30 days, 15:32:47
iter:163 loss:0.536 ETA:30 days, 10:06:50
iter:164 loss:0.656 ETA:30 days, 14:01:38
iter:165 loss:0.553 ETA:30 days, 21:14:49
iter:166 loss:0.439 ETA:30 days, 15:31:19
iter:167 loss:0.468 ETA:30 days, 10:38:05
iter:168 loss:0.527 ETA:30 days, 12:23:57
iter:169 loss:0.466 ETA:30 days, 14:23:12
iter:170 loss:0.535 ETA:30 days, 19:29:52 flow:0.108 pen:0.199 wNCE:4.089 gNCE:0.461 pen_acc:0.984 wW:0.050 wG:0.050
iter:171 loss:0.548 ETA:30 days, 16:02:59
iter:172 loss:0.416 ETA:30 days, 15:32:18
iter:173 loss:0.481 ETA:30 days, 10:07:12
iter:174 loss:0.500 ETA:30 days, 11:27:17
iter:175 loss:0.446 ETA:30 days, 16:03:20
iter:176 loss:0.413 ETA:31 days, 0:18:47
iter:177 loss:0.480 ETA:30 days, 12:04:08
iter:178 loss:0.438 ETA:30 days, 14:32:46
iter:179 loss:0.413 ETA:30 days, 10:07:58
iter:180 loss:0.455 ETA:30 days, 12:36:01 flow:0.068 pen:0.178 wNCE:3.768 gNCE:0.412 pen_acc:0.983 wW:0.050 wG:0.050
iter:181 loss:0.470 ETA:30 days, 15:57:31
iter:182 loss:0.412 ETA:30 days, 20:20:16
iter:183 loss:0.388 ETA:30 days, 15:24:18
iter:184 loss:0.406 ETA:30 days, 14:44:56
iter:185 loss:0.492 ETA:30 days, 12:14:57
iter:186 loss:0.460 ETA:30 days, 10:28:12
iter:187 loss:0.468 ETA:30 days, 15:30:28
iter:188 loss:0.529 ETA:31 days, 0:37:01
iter:189 loss:0.644 ETA:30 days, 10:42:17
iter:190 loss:0.519 ETA:30 days, 15:13:55 flow:0.164 pen:0.122 wNCE:4.136 gNCE:0.526 pen_acc:0.990 wW:0.050 wG:0.050
iter:191 loss:0.417 ETA:30 days, 12:02:43
iter:192 loss:0.622 ETA:30 days, 12:35:58
iter:193 loss:0.812 ETA:30 days, 19:00:14
iter:194 loss:0.883 ETA:30 days, 21:08:20
iter:195 loss:0.607 ETA:30 days, 12:11:57
iter:196 loss:0.556 ETA:30 days, 13:36:55
iter:197 loss:0.629 ETA:30 days, 11:25:29
iter:198 loss:0.587 ETA:30 days, 9:58:11
iter:199 loss:0.517 ETA:30 days, 11:48:40
[INF-chunk0] h=6, a.std=0.6150, pen_prob(mean)=[0.8772249221801758, 0.00714033842086792, 0.11563476175069809]
iter:200 loss:0.575 ETA:31 days, 2:31:58 flow:0.129 pen:0.226 wNCE:3.994 gNCE:0.403 pen_acc:0.983 wW:0.050 wG:0.050
iter:201 loss:0.597 ETA:30 days, 15:05:44
iter:202 loss:0.600 ETA:30 days, 10:48:14
iter:203 loss:0.464 ETA:30 days, 14:59:36
iter:204 loss:0.463 ETA:30 days, 11:20:16
iter:205 loss:0.512 ETA:30 days, 10:42:16
iter:206 loss:0.418 ETA:30 days, 16:09:39
iter:207 loss:0.507 ETA:30 days, 21:55:52
iter:208 loss:0.444 ETA:30 days, 11:42:11
iter:209 loss:0.452 ETA:30 days, 14:31:31
iter:210 loss:0.431 ETA:30 days, 12:07:22 flow:0.047 pen:0.165 wNCE:3.883 gNCE:0.491 pen_acc:0.985 wW:0.050 wG:0.050
iter:211 loss:0.401 ETA:30 days, 11:23:25
iter:212 loss:0.434 ETA:31 days, 1:44:50
iter:213 loss:0.463 ETA:30 days, 14:12:35
iter:214 loss:0.462 ETA:30 days, 10:29:35
iter:215 loss:0.438 ETA:30 days, 14:53:15
iter:216 loss:0.419 ETA:30 days, 12:42:21
iter:217 loss:0.470 ETA:30 days, 10:49:47
