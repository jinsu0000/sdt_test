[SEED] 1001
Filter the characters containing more than max_len points
Filter the characters containing more than max_len points
Number of train images: 50557 | test images: 12634
Content_TR:: __init__ d_model: 512 , nhead: 2 , num_encoder_layers: 3 , dim_feedforward: 2048 , dropout: 0.1 , activation: relu , normalize_before: True
[TransformerDecoderLayer]: d_model=512, nhead=8, dim_feedforward=2048, dropout=0.1, activation=relu, normalize_before=True
TransformerDecoder:: __init__ decoder_layer: TransformerDecoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (multihead_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (linear1): Linear(in_features=512, out_features=2048, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=2048, out_features=512, bias=True)
  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
  (dropout3): Dropout(p=0.1, inplace=False)
) , num_layers: 2 , norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True) , return_intermediate: True
TransformerDecoder:: __init__ decoder_layer: TransformerDecoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (multihead_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (linear1): Linear(in_features=512, out_features=2048, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=2048, out_features=512, bias=True)
  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
  (dropout3): Dropout(p=0.1, inplace=False)
) , num_layers: 2 , norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True) , return_intermediate: True
SDT_FlowWrapper:: __init__ H: 6, stride_default: 4, n_layers: 6, n_head: 8, ffn_mult: 4, p: 0.1
[TB] writing to: Saved/English_CASIA/flow_run-20251011_200756/tboard
[INFO] max_iter: 300000
/home/jinsu0000/anaconda3/envs/exp-sdt/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/jinsu0000/anaconda3/envs/exp-sdt/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Content_TR:: Feat_Encoder input: torch.Size([32, 1, 64, 64])
Content_TR:: Feat_Encoder Feat_Encoder(conv2d+resnet) output: torch.Size([32, 512, 2, 2])
Content_TR:: rearrange output: torch.Size([4, 32, 512])
Content_TR:: encoder output: torch.Size([4, 32, 512])
SDT_FlowWrapper::_content_token char_img: torch.Size([32, 1, 64, 64]), torch.float32 -> cont: torch.Size([32, 512]), torch.float32
SDT_FlowWrapper::flow_match_loss B: 32, T: 33, C: 5, H: 6, stride: 4, cond_ctx: torch.Size([32, 3, 512]), torch.float32
SDT_FlowWrapper::flow_match_loss B=32 T=33 H=6 stride=4 | EOC%=100.0 | first_eoc(mean)=31.0
iter:0 loss:6.741 ETA:60 days, 20:07:22 flow:5.261 pen:1.059 wNCE:4.114 gNCE:4.292 pen_acc:0.482 wW:0.050 wG:0.050
iter:1 loss:214.279 ETA:30 days, 17:01:56
iter:2 loss:20.815 ETA:29 days, 17:27:39
iter:3 loss:76.395 ETA:30 days, 12:06:40
iter:4 loss:28.327 ETA:29 days, 22:27:17
iter:5 loss:14.910 ETA:30 days, 4:25:04
iter:6 loss:7.939 ETA:30 days, 7:53:17
iter:7 loss:6.764 ETA:30 days, 12:38:35
iter:8 loss:5.559 ETA:30 days, 21:50:15
iter:9 loss:3.354 ETA:30 days, 8:46:51
iter:10 loss:8.145 ETA:30 days, 9:12:40 flow:7.309 pen:0.461 wNCE:4.203 gNCE:3.277 pen_acc:0.984 wW:0.050 wG:0.050
iter:11 loss:4.634 ETA:30 days, 22:56:47
iter:12 loss:3.556 ETA:30 days, 13:17:43
iter:13 loss:4.331 ETA:30 days, 8:41:11
iter:14 loss:3.595 ETA:30 days, 14:29:36
iter:15 loss:3.535 ETA:30 days, 10:02:17
iter:16 loss:3.844 ETA:30 days, 10:00:35
iter:17 loss:3.727 ETA:31 days, 5:05:50
iter:18 loss:3.675 ETA:30 days, 12:36:17
iter:19 loss:3.908 ETA:30 days, 9:28:51
iter:20 loss:4.515 ETA:30 days, 17:21:48 flow:3.725 pen:0.438 wNCE:4.064 gNCE:2.977 pen_acc:0.985 wW:0.050 wG:0.050
iter:21 loss:3.166 ETA:30 days, 12:56:59
iter:22 loss:4.162 ETA:30 days, 21:35:23
iter:23 loss:4.900 ETA:30 days, 18:11:41
iter:24 loss:4.913 ETA:30 days, 12:14:58
iter:25 loss:2.958 ETA:30 days, 13:24:45
iter:26 loss:3.656 ETA:30 days, 19:20:12
iter:27 loss:3.547 ETA:30 days, 15:45:59
iter:28 loss:2.865 ETA:30 days, 23:40:16
iter:29 loss:3.627 ETA:30 days, 14:49:54
iter:30 loss:3.028 ETA:30 days, 9:43:23 flow:2.331 pen:0.365 wNCE:4.032 gNCE:2.608 pen_acc:0.991 wW:0.050 wG:0.050
iter:31 loss:3.798 ETA:30 days, 10:53:02
iter:32 loss:2.789 ETA:30 days, 16:57:00
iter:33 loss:3.711 ETA:31 days, 0:47:40
iter:34 loss:4.640 ETA:30 days, 10:57:11
iter:35 loss:3.023 ETA:30 days, 15:28:13
iter:36 loss:3.766 ETA:30 days, 10:51:49
iter:37 loss:3.641 ETA:30 days, 10:28:22
iter:38 loss:3.450 ETA:30 days, 16:56:25
iter:39 loss:3.631 ETA:31 days, 1:31:56
iter:40 loss:3.549 ETA:30 days, 12:10:14 flow:2.840 pen:0.388 wNCE:4.111 gNCE:2.310 pen_acc:0.986 wW:0.050 wG:0.050
iter:41 loss:2.563 ETA:30 days, 11:41:28
iter:42 loss:4.360 ETA:30 days, 14:58:49
iter:43 loss:3.716 ETA:30 days, 12:05:19
iter:44 loss:2.917 ETA:30 days, 12:12:01
iter:45 loss:3.030 ETA:31 days, 3:15:27
iter:46 loss:4.096 ETA:30 days, 13:26:23
iter:47 loss:3.684 ETA:30 days, 12:17:17
iter:48 loss:2.113 ETA:30 days, 15:40:59
iter:49 loss:2.468 ETA:30 days, 14:12:15
iter:50 loss:2.579 ETA:30 days, 19:48:49 flow:1.905 pen:0.368 wNCE:4.019 gNCE:2.097 pen_acc:0.985 wW:0.050 wG:0.050
iter:51 loss:2.184 ETA:30 days, 21:50:23
iter:52 loss:2.139 ETA:30 days, 11:09:09
iter:53 loss:2.100 ETA:30 days, 12:02:30
iter:54 loss:1.958 ETA:30 days, 15:26:36
iter:55 loss:2.329 ETA:30 days, 16:18:17
iter:56 loss:2.011 ETA:31 days, 2:39:14
iter:57 loss:2.456 ETA:30 days, 15:11:21
iter:58 loss:2.077 ETA:30 days, 11:49:56
iter:59 loss:4.944 ETA:30 days, 13:22:17
iter:60 loss:1.954 ETA:30 days, 17:40:14 flow:1.378 pen:0.290 wNCE:3.951 gNCE:1.780 pen_acc:0.988 wW:0.050 wG:0.050
iter:61 loss:2.085 ETA:30 days, 11:56:36
iter:62 loss:2.818 ETA:31 days, 0:29:09
iter:63 loss:1.835 ETA:30 days, 17:16:36
iter:64 loss:2.726 ETA:30 days, 12:28:19
iter:65 loss:2.058 ETA:30 days, 13:10:18
iter:66 loss:2.638 ETA:30 days, 17:16:46
iter:67 loss:2.306 ETA:30 days, 13:40:49
iter:68 loss:2.420 ETA:30 days, 23:49:11
iter:69 loss:1.911 ETA:30 days, 17:16:56
iter:70 loss:1.776 ETA:30 days, 13:15:51 flow:1.253 pen:0.235 wNCE:3.987 gNCE:1.774 pen_acc:0.986 wW:0.050 wG:0.050
iter:71 loss:2.072 ETA:30 days, 11:27:31
iter:72 loss:2.003 ETA:30 days, 16:36:25
iter:73 loss:2.087 ETA:30 days, 18:08:23
iter:74 loss:2.034 ETA:30 days, 18:23:12
iter:75 loss:1.664 ETA:30 days, 11:27:41
iter:76 loss:2.010 ETA:30 days, 18:00:07
iter:77 loss:2.994 ETA:30 days, 13:03:27
iter:78 loss:2.479 ETA:30 days, 13:26:27
iter:79 loss:1.723 ETA:31 days, 5:30:20
iter:80 loss:2.555 ETA:30 days, 12:36:59 flow:2.042 pen:0.203 wNCE:4.037 gNCE:2.158 pen_acc:0.986 wW:0.050 wG:0.050
iter:81 loss:2.698 ETA:30 days, 13:56:36
iter:82 loss:2.379 ETA:30 days, 17:32:38
iter:83 loss:2.120 ETA:30 days, 15:40:29
iter:84 loss:1.854 ETA:30 days, 14:01:57
iter:85 loss:1.884 ETA:31 days, 5:20:06
iter:86 loss:2.076 ETA:30 days, 11:29:09
iter:87 loss:1.803 ETA:30 days, 17:09:33
iter:88 loss:2.018 ETA:30 days, 17:52:46
iter:89 loss:2.018 ETA:30 days, 15:07:42
iter:90 loss:2.103 ETA:31 days, 0:18:26 flow:1.674 pen:0.147 wNCE:4.151 gNCE:1.500 pen_acc:0.991 wW:0.050 wG:0.050
iter:91 loss:1.883 ETA:30 days, 23:24:44
iter:92 loss:1.710 ETA:30 days, 13:05:01
iter:93 loss:1.521 ETA:30 days, 13:02:03
iter:94 loss:1.758 ETA:30 days, 17:31:14
iter:95 loss:1.721 ETA:30 days, 15:21:20
iter:96 loss:2.378 ETA:31 days, 0:19:59
iter:97 loss:1.986 ETA:30 days, 19:47:58
iter:98 loss:2.611 ETA:30 days, 15:06:33
iter:99 loss:1.713 ETA:30 days, 15:15:39
SDT_FlowWrapper::flow_infer style_imgs: torch.Size([1, 30, 1, 64, 64]), torch.float32, B: 1, char_img: torch.Size([1, 1, 64, 64]), torch.float32, T: 120, steps: 20, stride: 4, replan: 4, solver: euler, micro_pen_ensemble: False, micro_pen_weight: linear
Content_TR:: Feat_Encoder input: torch.Size([1, 1, 64, 64])
Content_TR:: Feat_Encoder Feat_Encoder(conv2d+resnet) output: torch.Size([1, 512, 2, 2])
Content_TR:: rearrange output: torch.Size([4, 1, 512])
Content_TR:: encoder output: torch.Size([4, 1, 512])
SDT_FlowWrapper::_content_token char_img: torch.Size([1, 1, 64, 64]), torch.float32 -> cont: torch.Size([1, 512]), torch.float32
[INF-chunk0] h=6, a.std=1.1980, pen_prob(mean)=[0.9911772608757019, 0.007291185669600964, 0.0015316673088818789]
trainer_flow::train tb_sample() delta5.size(-1) = 5
iter:100 loss:1.910 ETA:30 days, 11:10:41 flow:1.486 pen:0.160 wNCE:3.966 gNCE:1.313 pen_acc:0.982 wW:0.050 wG:0.050
iter:101 loss:1.353 ETA:34 days, 3:02:59
iter:102 loss:1.732 ETA:34 days, 18:36:19
iter:103 loss:1.361 ETA:34 days, 8:58:41
iter:104 loss:2.238 ETA:30 days, 16:31:56
iter:105 loss:1.558 ETA:30 days, 14:18:51
iter:106 loss:6.983 ETA:31 days, 6:15:44
iter:107 loss:3.538 ETA:30 days, 5:41:09
iter:108 loss:1.565 ETA:31 days, 6:59:29
iter:109 loss:1.719 ETA:30 days, 15:28:45
iter:110 loss:1.798 ETA:30 days, 18:07:20 flow:1.380 pen:0.169 wNCE:4.026 gNCE:0.957 pen_acc:0.983 wW:0.050 wG:0.050
iter:111 loss:2.146 ETA:30 days, 16:02:36
iter:112 loss:1.715 ETA:30 days, 15:13:20
iter:113 loss:1.696 ETA:30 days, 23:33:45
iter:114 loss:1.501 ETA:31 days, 5:04:38
iter:115 loss:1.977 ETA:30 days, 15:20:02
iter:116 loss:1.840 ETA:30 days, 18:12:11
iter:117 loss:2.130 ETA:30 days, 15:49:23
iter:118 loss:1.620 ETA:30 days, 15:20:01
iter:119 loss:1.535 ETA:31 days, 9:16:05
iter:120 loss:1.521 ETA:30 days, 16:51:47 flow:1.053 pen:0.225 wNCE:4.000 gNCE:0.867 pen_acc:0.982 wW:0.050 wG:0.050
iter:121 loss:1.966 ETA:30 days, 15:44:43
iter:122 loss:1.508 ETA:30 days, 19:36:33
iter:123 loss:1.897 ETA:30 days, 16:39:17
iter:124 loss:1.570 ETA:30 days, 14:38:18
iter:125 loss:1.485 ETA:31 days, 9:14:11
iter:126 loss:1.310 ETA:30 days, 12:35:24
iter:127 loss:1.642 ETA:30 days, 14:51:49
iter:128 loss:1.620 ETA:30 days, 19:15:53
iter:129 loss:1.607 ETA:30 days, 16:54:02
iter:130 loss:1.512 ETA:30 days, 23:43:29 flow:1.089 pen:0.166 wNCE:4.044 gNCE:1.105 pen_acc:0.986 wW:0.050 wG:0.050
iter:131 loss:1.759 ETA:30 days, 23:05:02
iter:132 loss:1.349 ETA:30 days, 14:13:16
iter:133 loss:1.763 ETA:30 days, 14:30:11
iter:134 loss:1.841 ETA:30 days, 18:19:34
iter:135 loss:1.965 ETA:30 days, 15:24:25
iter:136 loss:1.695 ETA:31 days, 5:45:41
iter:137 loss:1.825 ETA:30 days, 19:57:02
iter:138 loss:2.307 ETA:30 days, 14:54:41
iter:139 loss:2.000 ETA:30 days, 14:10:23
iter:140 loss:1.464 ETA:30 days, 16:34:14 flow:1.035 pen:0.203 wNCE:3.821 gNCE:0.704 pen_acc:0.981 wW:0.050 wG:0.050
